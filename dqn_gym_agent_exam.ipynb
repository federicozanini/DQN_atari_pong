{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Atari2600 Pong with Reinforcement Learning and Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Input, Flatten\n",
    "import cv2 as cv\n",
    "import gym\n",
    "import gnwrapper\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model directory\n",
    "\n",
    "if not os.path.exists('./models'):\n",
    "    os.mkdir('./models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, replay_memory_size, sample_size):\n",
    "        \n",
    "        self.memorySize = replay_memory_size       #How many transitions store\n",
    "        self.sampleSize=sample_size                #How many transitions (s, a, r, s1, d) pick (mini_batch)\n",
    "        self.replay_memory = deque(maxlen=replay_memory_size)\n",
    "        \n",
    "    \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        if len(self.replay_memory) ==  self.memorySize:\n",
    "                self.replay_memory.popleft() # delete the first tuple\n",
    "                \n",
    "        state = np.array(state)\n",
    "        next_state = np.array(next_state)\n",
    "        \n",
    "        if done:\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "            \n",
    "        tmp = (state, action, reward, next_state, done)\n",
    "                \n",
    "        self.replay_memory.append(tmp)\n",
    "        \n",
    "    \n",
    "    # taken from lectures' code   \n",
    "    def sample_experience(self): \n",
    "        \n",
    "        batch = []\n",
    "        random_samples_idxs = np.random.choice(range(len(self.replay_memory)), size=self.sampleSize)\n",
    "\n",
    "        for idx in random_samples_idxs:\n",
    "            batch.append(self.replay_memory[idx])\n",
    "            \n",
    "        states, actions, rewards, next_states, dones = [np.array([ex[field_index] for ex in batch]) \n",
    "                                                        for field_index in range(5)]\n",
    "        \n",
    "\n",
    "        return states, actions, rewards, next_states, dones \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, n_actions, frame_stack, img_width, img_height, lrn_rate, batch_size, epsilon):\n",
    "        \n",
    "        self.n_actions           = n_actions\n",
    "        self.img_width           = img_width\n",
    "        self.img_height          = img_height\n",
    "        self.frame_stack         = frame_stack\n",
    "        \n",
    "        self.batch_size          = batch_size\n",
    "        self.lrn_rate            = lrn_rate\n",
    "        self.gamma               = 0.99          #discount factor\n",
    "\n",
    "        self.epsilon             = epsilon\n",
    "        self.epsilon_max         = 1.0\n",
    "        self.epsilon_min         = 0.01\n",
    "        self.epsilon_decrease    = 100000.0\n",
    "        \n",
    "        self.frame_count         = 0\n",
    "        \n",
    "        self.model_Q             = self.build_model()\n",
    "        self.model_Q_target      = self.build_model()\n",
    "        \n",
    "        self.loss                = tf.keras.losses.Huber()\n",
    "        self.opt                 = tf.keras.optimizers.Adam(learning_rate=self.lrn_rate)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "\n",
    "        inputs = Input(shape=(\n",
    "                            self.frame_stack, \n",
    "                            self.img_width, \n",
    "                            self.img_height, \n",
    "                            1\n",
    "                            ), \n",
    "                            batch_size = self.batch_size, \n",
    "                            name = \"input\")\n",
    "        \n",
    "                       \n",
    "        conv_1 = Conv2D(16,\n",
    "                        8,\n",
    "                        strides = 4,\n",
    "                        activation = \"relu\")(inputs)\n",
    "\n",
    "        conv_2 = Conv2D(32,\n",
    "                        4,\n",
    "                        strides = 2,\n",
    "                        activation = \"relu\")(conv_1)\n",
    "\n",
    "        flatten = Flatten()(conv_2)\n",
    "        \n",
    "        dense = Dense(256, activation=\"relu\")(flatten)\n",
    "        \n",
    "        output = Dense(self.n_actions, activation=\"linear\", name= \"output\")(dense)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=output)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def policy_action_selection(self, state):\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        \n",
    "        else:\n",
    "            action_probability = self.model_Q(tf.expand_dims(state, 0), training=False)\n",
    "            return tf.argmax(action_probability[0]).numpy()\n",
    "        \n",
    "        \n",
    "    def epsilon_decrease(self):\n",
    "        \n",
    "        if frame_count <= exploration_frames:\n",
    "            self.epsilon = self.epsilon - ( (self.epsilon_max - self.epsilon_min) / self.epsilon_decrease )\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "            \n",
    "    \n",
    "    def training(self, experience_replay: ExperienceReplay):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experience_replay.sample_experience()\n",
    "        \n",
    "        target_q_values = self.model_Q_target(next_states)\n",
    "        max_target_q_values = tf.reduce_max(target_q_values, axis=1)\n",
    "        \n",
    "        #if state is terminal: y_target = r_j\n",
    "        #otherwise: y_target = r_j + gamma * max_target_q_value\n",
    "        updated_target_q_values = rewards + (self.gamma * max_target_q_values * (1 - dones))\n",
    "        \n",
    "        #encode the actions\n",
    "        actions_one_hot = tf.one_hot(actions, self.n_actions)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model_Q(states)\n",
    "\n",
    "            # select the q_value for the action taken using the mask\n",
    "            q_values = tf.reduce_sum(q_values * actions_one_hot, axis=1, keepdims=True)\n",
    "\n",
    "            # calculate loss on target q_value and q_values for the action taken\n",
    "            loss = self.loss(updated_target_q_values, q_values)\n",
    "\n",
    "        # we calculate the gradient and apply it to model_Q\n",
    "        grads = tape.gradient(loss, self.model_Q.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model_Q.trainable_variables))\n",
    "        \n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.model_Q_target.set_weights(self.model_Q.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "1. Load the environment\n",
    "2. Use wrapper for image preprocessing\n",
    "3. Use wrapper for frame stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(name, frame_skip, frame_stack, screen_size, grayscale_obs, show_env_spec):\n",
    "    \n",
    "    env = gym.make(name)\n",
    "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=frame_skip, screen_size=screen_size, grayscale_obs=grayscale_obs)\n",
    "    env = gym.wrappers.FrameStack(env, frame_stack)\n",
    "    \n",
    "    if show_env_spec:\n",
    "        obs = np.array(env.reset())\n",
    "        print(obs.shape)\n",
    "        plt.imshow(obs[3])\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT    = \"PongNoFrameskip-v4\"\n",
    "\n",
    "frame_skip     = 4\n",
    "frame_stack    = 4\n",
    "screen_size    = 84\n",
    "grayscale_obs  = True\n",
    "show_env_spec  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_environment(\n",
    "                       name = ENVIRONMENT,\n",
    "                       frame_skip = frame_skip,\n",
    "                       frame_stack = frame_stack,\n",
    "                       screen_size = screen_size,\n",
    "                       grayscale_obs = grayscale_obs,\n",
    "                       show_env_spec = show_env_spec\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "env_actions = env.unwrapped.get_action_meanings()\n",
    "print(env_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1. Set hyperparameters\n",
    "2. Set the DQN Agent\n",
    "3. Set the Experience replay\n",
    "4. Start the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count         = 0\n",
    "episode_reward        = 0\n",
    "episode_max_steps     = 10000\n",
    "episode_best_reward   = 0\n",
    "\n",
    "frame_count           = 0\n",
    "exploration_frames    = 100000\n",
    "\n",
    "episode_reward_buffer = []\n",
    "history_reward_buffer = []\n",
    "best_avg_reward       = -100\n",
    "avg_reward            = 0\n",
    "\n",
    "C                     = 1000 #frequency update target network\n",
    "training_frequency    = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN = DQNAgent(\n",
    "               n_actions=n_actions,\n",
    "               frame_stack=frame_stack,\n",
    "               img_width=screen_size,\n",
    "               img_height=screen_size,\n",
    "               lrn_rate=1e-4,\n",
    "               batch_size=32,\n",
    "               epsilon=1.0\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExpR = ExperienceReplay(\n",
    "                        replay_memory_size=10000,\n",
    "                        sample_size=32\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range (1, episode_max_steps):\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            #select action following policy\n",
    "            action = DQN.policy_action_selection(state)\n",
    "            \n",
    "            #decrease epsilon value\n",
    "            if frame_count <= exploration_frames:\n",
    "                DQN.epsilon = DQN.epsilon - ( (DQN.epsilon_max - DQN.epsilon_min) / DQN.epsilon_decrease )\n",
    "            else:\n",
    "                DQN.epsilon = DQN.epsilon_min\n",
    "            \n",
    "            #step the policy action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #store transition in experience replay\n",
    "            ExpR.add_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            #update state\n",
    "            state = next_state\n",
    "            \n",
    "            #add reward\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if len(ExpR.replay_memory) > 32:\n",
    "                DQN.training(ExpR)\n",
    "                \n",
    "            if frame_count % C == 0:\n",
    "                DQN.update_target_network()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        ### END OF EPISODE ROUTINE ### \n",
    "        \n",
    "        episode_count += 1\n",
    "        \n",
    "        episode_reward_buffer.append(episode_reward)\n",
    "        history_reward_buffer.append(episode_reward)\n",
    "        \n",
    "        avg_reward = np.mean(episode_reward_buffer)\n",
    "        avg_history_reward = np.mean(history_reward_buffer)\n",
    "\n",
    "        # print update of the training at the end of the episode\n",
    "        now = datetime.now()\n",
    "        time_string = now.strftime(\"%H:%M:%S\")\n",
    "        print(\"Episode {}, h_rwrd {:.2f}, avg_r: {:.2f}, episode reward {}, frame_count {}, eps= {:.4f}, ({})\".format(episode_count, avg_history_reward, avg_reward, episode_reward, frame_count, DQN.epsilon, time_string))\n",
    "        \n",
    "        with open(\"training_output.txt\", \"a\") as output_file:\n",
    "            output_file.write(\"Episode {}, h_rwrd {:.2f}, avg_r: {:.2f}, episode reward {}, frame_count {}, eps= {:.4f}, ({})\\n\".format(episode_count, avg_history_reward, avg_reward, episode_reward, frame_count, DQN.epsilon, time_string))\n",
    "            \n",
    "        if episode_count % 100 == 0:\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                episode_reward_buffer = []\n",
    "                \n",
    "                now = datetime.now()\n",
    "                time_string = now.strftime(\"%H:%M:%S\")\n",
    "                print(\"+++++++++ New Best Average Reward Model: {:.2f} at episode {}, ({}) +++++++++\".format(best_avg_reward, episode_count, time_string))\n",
    "                DQN.model_Q.save('./models/model_Q_'+ str(best_avg_reward))\n",
    "                print(\"Saved the model that holds the record...\")\n",
    "                DQN.model_Q.save('./models/model_Q')\n",
    "                DQN.model_Q_target.save('./models/model_Q_target')\n",
    "                \n",
    "            else:\n",
    "                episode_reward_buffer = []\n",
    "\n",
    "        # we save the models every 30 episodes\n",
    "        if episode_count % 30 == 0:\n",
    "            DQN.model_Q.save('./models/model_Q')\n",
    "            DQN.model_Q_target.save('./models/model_Q_target')\n",
    "        \n",
    "except Exception as e: \n",
    "    print(\"Exception raised, saving the models...\")\n",
    "    print(e)\n",
    "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "    print(exc_type, exc_tb.tb_lineno)\n",
    "    print()\n",
    "    DQN.model_Q.save('./models/model_Q')\n",
    "    DQN.model_Q_target.save('./models/model_Q_target')   \n",
    "\n",
    "except KeyboardInterrupt as ki:\n",
    "    print(\"Execution terminated manually, saving the models...\")\n",
    "    print(ki)\n",
    "    DQN.model_Q.save('./models/model_Q')\n",
    "    DQN.model_Q_target.save('./models/model_Q_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "1. Set the environment\n",
    "2. Evaluate model performances over 10 episodes\n",
    "\n",
    "(The evaluation has been made on Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup\n",
    "\n",
    "#!(apt update && apt install xvfb ffmpeg python-opengl -y) > /dev/null 2>&1\n",
    "#!pip install gym-notebook-wrapper > /dev/null 2>&1\n",
    "#!pip install opencv-python > /dev/null 2>&1\n",
    "#!pip install gym[atari] > /dev/null 2>&1\n",
    "#!pip install lz4 > /dev/null 2>&1\n",
    "#!wget http://www.atarimania.com/roms/Roms.rar\n",
    "#!unrar e Roms.rar\n",
    "#!python -m atari_py.import_roms ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT    = \"PongNoFrameskip-v4\"\n",
    "\n",
    "frame_skip     = 4\n",
    "frame_stack    = 4\n",
    "screen_size    = 84\n",
    "grayscale_obs  = True\n",
    "show_env_spec  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_environment(\n",
    "                       name = ENVIRONMENT,\n",
    "                       frame_skip = frame_skip,\n",
    "                       frame_stack = frame_stack,\n",
    "                       screen_size = screen_size,\n",
    "                       grayscale_obs = grayscale_obs,\n",
    "                       show_env_spec = show_env_spec\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Q = keras.models.load_model('/Users/federicozanini/Desktop/2022_aas/models/18.59_dqn_gym_agent/model')\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "env = gnwrapper.Monitor(env, './videos', video_callable=lambda episode_id: True,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "returns = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    ret = 0\n",
    "    state = np.array(env.reset())\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        action_probs = model_Q(tf.expand_dims(state, 0), training=False)        \n",
    "\n",
    "        # predict the best action\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # step action into the environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        # set the new state as state for the next prediction\n",
    "        state = state_next\n",
    "\n",
    "        # accumulate the reward\n",
    "        ret += reward\n",
    "\n",
    "    if ret >= 20.0:    \n",
    "        env.display()\n",
    "        break\n",
    "    else:\n",
    "        dir = './videos'\n",
    "        for f in os.listdir(dir):\n",
    "            os.remove(os.path.join(dir, f))\n",
    "\n",
    "        \n",
    "    print(\"Episode {} terminated with reward {}\".format(i, ret))\n",
    "  # store the episode reward\n",
    "    returns.append(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
